import math
import argparse
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from model.BNN_improve_去掉光谱增强 import BayesianNeuralNetwork, bnn_loss, get_kl_weight, adaptive_kl_weight
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from utils import seed_torch
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import time
import random
import gc

seed_torch()
device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")


def clear_gpu_memory():
    """清理GPU内存"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()


class CustomDataset(Dataset):
    def __init__(self, label_df, data_dir, augmentation=None):
        self.label_df = label_df
        self.data_dir = data_dir
        # 注意：保留augmentation参数以保持接口一致，但不使用

    def __len__(self):
        return len(self.label_df)

    def __getitem__(self, idx):
        file_name = self.label_df.iloc[idx, 0]
        label = self.label_df.iloc[idx, 1:6].values.astype('float32')

        file_path = os.path.join(self.data_dir, str(file_name) + '.csv')
        data = pd.read_csv(file_path, header=None).values.astype('float32').flatten()

        data = torch.tensor(data, dtype=torch.float32)
        label = torch.tensor(label, dtype=torch.float32)
        data = torch.unsqueeze(data, 0)

        # 移除数据增强功能，直接返回原始数据
        # if self.augmentation is not None:
        #     data = self.augmentation(data)

        return data, label


def calculate_catastrophic_outliers_ratio(true_values, predictions, threshold=3.0):
    """计算灾难性异常值比例 η"""
    relative_errors = np.abs(predictions - true_values) / np.abs(true_values)
    relative_errors = np.where(np.abs(true_values) < 1e-8, 0, relative_errors)

    eta_per_param = []
    for i in range(true_values.shape[1]):
        catastrophic_count = np.sum(relative_errors[:, i] > threshold)
        eta = catastrophic_count / len(relative_errors)
        eta_per_param.append(eta)

    return np.array(eta_per_param)


def train_one_epoch_bnn(model, optimizer, scheduler, data_loader, device, epoch, n_train_samples,
                        kl_weight=1e-6, l2_weight=1e-7, total_epochs=200):
    model.train()
    running_total_loss = 0.0
    running_nll_loss = 0.0
    running_kl_loss = 0.0
    running_mse = 0.0

    dynamic_kl_weight = get_kl_weight(epoch, total_epochs, strategy='aggressive')

    for step, data in enumerate(data_loader):
        spectra, labels = data
        spectra = spectra.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        # 前向传播
        mu, noise_var = model(spectra)

        # 计算KL散度
        kl_divergence = model.kl_divergence()

        # BNN损失
        total_loss, nll_loss, kl_loss = bnn_loss(
            mu, noise_var, labels, kl_divergence, n_train_samples,
            dynamic_kl_weight, l2_weight, model
        )

        # 反向传播
        total_loss.backward()

        # 梯度裁剪
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # OneCycleLR每个step调用
        if isinstance(scheduler, lr_scheduler.OneCycleLR):
            scheduler.step()

        # 统计
        running_total_loss += total_loss.item()
        running_nll_loss += nll_loss.item()
        running_kl_loss += kl_loss.item()

        mse = torch.mean((mu - labels) ** 2)
        running_mse += mse.item()

        # 定期清理内存
        if step % 20 == 0:
            clear_gpu_memory()

        if step % 50 == 0:
            print(f"Epoch [{epoch}], Step [{step}/{len(data_loader)}], "
                  f"Total Loss: {total_loss.item():.6f}, "
                  f"NLL: {nll_loss.item():.6f}, "
                  f"KL: {kl_loss.item():.6f}, "
                  f"KL_weight: {dynamic_kl_weight:.2e}, "
                  f"MSE: {mse.item():.6f}, "
                  f"Grad_norm: {grad_norm:.4f}")

    avg_total_loss = running_total_loss / len(data_loader)
    avg_nll_loss = running_nll_loss / len(data_loader)
    avg_kl_loss = running_kl_loss / len(data_loader)
    avg_mse = running_mse / len(data_loader)

    return avg_total_loss, avg_nll_loss, avg_kl_loss, avg_mse


def evaluate_bnn(model, data_loader, device, epoch, n_train_samples,
                 kl_weight=1e-6, l2_weight=1e-7, total_epochs=200):
    model.train()
    running_total_loss = 0.0
    running_nll_loss = 0.0
    running_kl_loss = 0.0
    running_mse = 0.0

    dynamic_kl_weight = get_kl_weight(epoch, total_epochs, strategy='aggressive')

    with torch.no_grad():
        for step, data in enumerate(data_loader):
            spectra, labels = data
            spectra = spectra.to(device)
            labels = labels.to(device)

            mu, noise_var = model(spectra)
            kl_divergence = model.kl_divergence()

            total_loss, nll_loss, kl_loss = bnn_loss(
                mu, noise_var, labels, kl_divergence, n_train_samples,
                dynamic_kl_weight, l2_weight, model
            )

            running_total_loss += total_loss.item()
            running_nll_loss += nll_loss.item()
            running_kl_loss += kl_loss.item()

            mse = torch.mean((mu - labels) ** 2)
            running_mse += mse.item()

            # 定期清理内存
            if step % 10 == 0:
                clear_gpu_memory()

    avg_total_loss = running_total_loss / len(data_loader)
    avg_nll_loss = running_nll_loss / len(data_loader)
    avg_kl_loss = running_kl_loss / len(data_loader)
    avg_mse = running_mse / len(data_loader)

    print(f"Validation - Epoch [{epoch}], "
          f"Total Loss: {avg_total_loss:.6f}, "
          f"NLL: {avg_nll_loss:.6f}, "
          f"KL: {avg_kl_loss:.6f}, "
          f"MSE: {avg_mse:.6f}")

    return avg_total_loss, avg_nll_loss, avg_kl_loss, avg_mse


def simple_calibrate_temperature(model, cal_loader, device):
    """简化的温度校准函数 - 避免内存问题"""
    print("Using simplified temperature calibration...")

    model.eval()
    predictions_list = []
    labels_list = []

    # 收集少量数据进行校准
    sample_count = 0
    max_samples = 64  # 限制样本数量

    with torch.no_grad():
        for data in cal_loader:
            if sample_count >= max_samples:
                break

            spectra, labels = data
            batch_size = min(8, spectra.size(0))  # 小批次
            spectra = spectra[:batch_size].to(device)
            labels = labels[:batch_size].to(device)

            # 单次前向传播
            mu, noise_var = model(spectra)

            predictions_list.append(mu.cpu())
            labels_list.append(labels.cpu())
            sample_count += batch_size

            # 清理内存
            clear_gpu_memory()

    if predictions_list:
        predictions = torch.cat(predictions_list, dim=0)
        labels = torch.cat(labels_list, dim=0)

        # 简单的温度设置
        mse_per_param = torch.mean((predictions - labels) ** 2, dim=0)

        # 基于MSE设置温度参数
        with torch.no_grad():
            temperature_vals = torch.sqrt(mse_per_param) + 0.5
            model.temperature_scaling.temperature.data = temperature_vals.to(device)
    else:
        # 默认温度设置
        with torch.no_grad():
            model.temperature_scaling.temperature.data.fill_(1.0)

    print(f"Temperature calibration completed. Temperatures: {model.temperature_scaling.temperature.data}")


def final_evaluate_bnn(model, data_loader, device, n_samples=50):  # 减少采样次数
    model.eval()
    all_predictions = []
    all_labels = []
    all_epistemic = []
    all_aleatoric = []
    all_total_uncertainty = []

    with torch.no_grad():
        for step, data in enumerate(data_loader):
            spectra, labels = data
            spectra = spectra.to(device)
            labels = labels.to(device)

            # 分小批次处理，避免内存问题
            batch_size = spectra.size(0)
            if batch_size > 16:
                # 分批处理
                mini_batch_predictions = []
                mini_batch_labels = []
                mini_batch_epistemic = []
                mini_batch_aleatoric = []
                mini_batch_total_uncertainty = []

                for i in range(0, batch_size, 16):
                    end_idx = min(i + 16, batch_size)
                    mini_spectra = spectra[i:end_idx]
                    mini_labels = labels[i:end_idx]

                    mean_pred, epistemic, aleatoric, total_unc = model.predict_with_uncertainty(
                        mini_spectra, n_samples=n_samples, use_calibration=True
                    )

                    mini_batch_predictions.append(mean_pred.detach().cpu())
                    mini_batch_labels.append(mini_labels.detach().cpu())
                    mini_batch_epistemic.append(epistemic.detach().cpu())
                    mini_batch_aleatoric.append(aleatoric.detach().cpu())
                    mini_batch_total_uncertainty.append(total_unc.detach().cpu())

                    # 清理内存
                    clear_gpu_memory()

                # 合并小批次结果
                all_predictions.append(torch.cat(mini_batch_predictions, dim=0))
                all_labels.append(torch.cat(mini_batch_labels, dim=0))
                all_epistemic.append(torch.cat(mini_batch_epistemic, dim=0))
                all_aleatoric.append(torch.cat(mini_batch_aleatoric, dim=0))
                all_total_uncertainty.append(torch.cat(mini_batch_total_uncertainty, dim=0))
            else:
                # 小批次直接处理
                mean_pred, epistemic, aleatoric, total_unc = model.predict_with_uncertainty(
                    spectra, n_samples=n_samples, use_calibration=True
                )

                all_predictions.append(mean_pred.detach().cpu())
                all_labels.append(labels.detach().cpu())
                all_epistemic.append(epistemic.detach().cpu())
                all_aleatoric.append(aleatoric.detach().cpu())
                all_total_uncertainty.append(total_unc.detach().cpu())

            # 定期清理内存
            if step % 5 == 0:
                clear_gpu_memory()

    # 转换为numpy数组
    predictions = torch.cat(all_predictions, dim=0).numpy()
    labels = torch.cat(all_labels, dim=0).numpy()
    epistemic = torch.cat(all_epistemic, dim=0).numpy()
    aleatoric = torch.cat(all_aleatoric, dim=0).numpy()
    total_uncertainty = torch.cat(all_total_uncertainty, dim=0).numpy()

    result = np.concatenate([
        labels,  # 真实值
        predictions,  # 预测值
        np.sqrt(epistemic),  # 认知不确定性
        np.sqrt(aleatoric),  # 偶然不确定性
        np.sqrt(total_uncertainty)  # 总不确定性
    ], axis=1)

    return result


def plot_training_curves(train_losses, val_losses, save_path):
    """绘制训练曲线"""
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (Log Scale)')
    plt.title('Training and Validation Loss (Log Scale)')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()


# 主训练循环
if __name__ == "__main__":
    # 创建必要的目录
    os.makedirs("./weights", exist_ok=True)
    os.makedirs("./results", exist_ok=True)
    os.makedirs("./plots", exist_ok=True)

    base_data_path = "./data/"
    result = np.array([[0] * 25])

    fold_results = []

    # 移除数据增强对象创建
    # augmentation = SpectralDataAugmentation(noise_std=0.003, shift_range=2)

    for fold_idx, fold in enumerate(["fold_1_", "fold_2_", "fold_3_", "fold_4_", "fold_5_"]):
        print(f"\n{'=' * 50}")
        print(f"Training on {fold}")
        print(f"{'=' * 50}")

        # 清理内存
        clear_gpu_memory()

        # 创建改进的BNN模型
        model = BayesianNeuralNetwork(
            in_channel=1,
            out_channel=5,
            spectrum_size=3516,
            hidden_dims=[512, 256, 128]  # 保持合理的网络大小
        )

        # 改进的优化器配置
        pg = [p for p in model.parameters() if p.requires_grad]
        optimizer = optim.AdamW(
            pg,
            lr=0.0015,  # 稍微降低学习率
            weight_decay=1e-5,
            betas=(0.9, 0.999),
            eps=1e-8
        )

        model.to(device)

        # 数据加载
        train_label = base_data_path + fold + 'train_labels.csv'
        data_dir = base_data_path + "data/"
        eval_label = base_data_path + fold + 'val_labels.csv'

        label_df = pd.read_csv(train_label)
        eval_label_df = pd.read_csv(eval_label)

        # 训练集和验证集都不使用数据增强
        traindataset = CustomDataset(label_df, data_dir, augmentation=None)
        evaldataset = CustomDataset(eval_label_df, data_dir, augmentation=None)

        # 减小批次大小避免内存问题
        data_loader = DataLoader(traindataset, batch_size=24, shuffle=True, num_workers=2, pin_memory=False)
        eval_data_loader = DataLoader(evaldataset, batch_size=24, num_workers=2, pin_memory=False)

        n_train_samples = len(traindataset)

        # 学习率调度器
        scheduler = lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=0.003,
            epochs=200,  # 稍微缩短训练
            steps_per_epoch=len(data_loader),
            pct_start=0.3,
            anneal_strategy='cos'
        )

        # 训练循环
        train_losses = []
        eval_losses = []
        train_mse_history = []
        eval_mse_history = []
        min_loss = float('inf')
        patience = 0
        max_patience = 35

        total_epochs = 200
        start_time = time.time()

        print(f"Starting training for {total_epochs} epochs...")
        print(f"Training samples: {n_train_samples}")
        print(f"Validation samples: {len(evaldataset)}")

        for epoch in range(total_epochs):
            epoch_start_time = time.time()

            # 训练
            total_loss, nll_loss, kl_loss, train_mse = train_one_epoch_bnn(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                data_loader=data_loader,
                device=device,
                epoch=epoch,
                n_train_samples=n_train_samples,
                total_epochs=total_epochs
            )
            train_losses.append(total_loss)
            train_mse_history.append(train_mse)

            # 验证
            val_total_loss, val_nll_loss, val_kl_loss, val_mse = evaluate_bnn(
                model=model,
                data_loader=eval_data_loader,
                device=device,
                epoch=epoch,
                n_train_samples=n_train_samples,
                total_epochs=total_epochs
            )
            eval_losses.append(val_total_loss)
            eval_mse_history.append(val_mse)

            # 保存最佳模型和早停
            if val_total_loss < min_loss:
                min_loss = val_total_loss
                torch.save(model.state_dict(), f"./weights/bestmodel_bnn_{fold}optimized.pth")
                print(f"New best model saved with loss: {min_loss:.6f}")
                patience = 0
            else:
                patience += 1

            epoch_time = time.time() - epoch_start_time
            print(f"Epoch {epoch} completed in {epoch_time:.2f}s")

            # 定期清理内存
            if epoch % 10 == 0:
                clear_gpu_memory()

            if patience >= max_patience:
                print(f"Early stopping at epoch {epoch}")
                break

        training_time = time.time() - start_time
        print(f"\nTraining completed in {training_time:.2f}s")

        # 绘制训练曲线
        plot_training_curves(train_losses, eval_losses, f"./plots/training_curve_{fold}.png")

        # 加载最佳模型
        model.load_state_dict(torch.load(f"./weights/bestmodel_bnn_{fold}optimized.pth"))

        # 简化的温度校准
        print("Performing temperature calibration...")
        simple_calibrate_temperature(model, eval_data_loader, device)

        # 最终评估
        print("Final evaluation with uncertainty quantification...")
        fold_result = final_evaluate_bnn(model, eval_data_loader, device, n_samples=50)
        fold_results.append(fold_result)

        # 保存单个fold的结果
        np.savetxt(f"./results/bnn_results_{fold}.csv", fold_result, delimiter=',',
                   fmt='%.6f', header='True1,True2,True3,True4,True5,'
                                      'Pred1,Pred2,Pred3,Pred4,Pred5,'
                                      'Epis1,Epis2,Epis3,Epis4,Epis5,'
                                      'Alea1,Alea2,Alea3,Alea4,Alea5,'
                                      'Total1,Total2,Total3,Total4,Total5')

        # 计算当前fold的性能指标
        true_values = fold_result[:, :5]
        predictions = fold_result[:, 5:10]

        mse = np.mean((true_values - predictions) ** 2, axis=0)
        mae = np.mean(np.abs(true_values - predictions), axis=0)
        bias = np.mean(predictions - true_values, axis=0)
        eta = calculate_catastrophic_outliers_ratio(true_values, predictions, threshold=3.0)

        print(f"\n{fold} Results:")
        print(f"MSE per parameter: {mse}")
        print(f"MAE per parameter: {mae}")
        print(f"Bias per parameter: {bias}")
        print(f"Eta (η) per parameter: {eta}")
        print(f"Average MSE: {np.mean(mse):.6f}")
        print(f"Average MAE: {np.mean(mae):.6f}")
        print(f"Overall Bias: {np.mean(bias):.6f}")
        print(f"Overall Eta (η): {np.mean(eta):.6f}")

        # 累积结果
        if fold_idx == 0:
            result = fold_result
        else:
            result = np.vstack([result, fold_result])

        # 清理内存
        del model
        clear_gpu_memory()

    # 保存所有结果
    np.savetxt("./results/bnn_all_results_optimized.csv", result, delimiter=',',
               fmt='%.6f', header='True1,True2,True3,True4,True5,'
                                  'Pred1,Pred2,Pred3,Pred4,Pred5,'
                                  'Epis1,Epis2,Epis3,Epis4,Epis5,'
                                  'Alea1,Alea2,Alea3,Alea4,Alea5,'
                                  'Total1,Total2,Total3,Total4,Total5')

    # 计算总体性能
    print(f"\n{'=' * 50}")
    print("OVERALL RESULTS")
    print(f"{'=' * 50}")

    true_all = result[:, :5]
    pred_all = result[:, 5:10]

    overall_mse = np.mean((true_all - pred_all) ** 2, axis=0)
    overall_mae = np.mean(np.abs(true_all - pred_all), axis=0)
    overall_std = np.std((true_all - pred_all) ** 2, axis=0)
    overall_bias = np.mean(pred_all - true_all, axis=0)
    overall_eta = calculate_catastrophic_outliers_ratio(true_all, pred_all, threshold=3.0)

    param_names = ['E(B-V)', '12+log(O/H)', 'R3', 'SFR', 'stellar mass']

    for i, param in enumerate(param_names):
        print(f"{param}: MSE={overall_mse[i]:.6f}, MAE={overall_mae[i]:.6f}, "
              f"STD={overall_std[i]:.6f}, Bias={overall_bias[i]:.6f}, η={overall_eta[i]:.6f}")

    print(f"\nAverage MSE: {np.mean(overall_mse):.6f}")
    print(f"Average MAE: {np.mean(overall_mae):.6f}")
    print(f"Average Bias: {np.mean(overall_bias):.6f}")
    print(f"Average Eta (η): {np.mean(overall_eta):.6f}")

    # 分析不确定性
    epistemic_all = result[:, 10:15]
    aleatoric_all = result[:, 15:20]
    total_uncertainty_all = result[:, 20:25]

    print(f"\nUncertainty Analysis:")
    print(f"Average Epistemic Uncertainty: {np.mean(epistemic_all, axis=0)}")
    print(f"Average Aleatoric Uncertainty: {np.mean(aleatoric_all, axis=0)}")
    print(f"Average Total Uncertainty: {np.mean(total_uncertainty_all, axis=0)}")

    # 绘制整体结果分析图
    plt.figure(figsize=(15, 10))

    # 预测 vs 真实值
    for i in range(5):
        plt.subplot(2, 3, i + 1)
        plt.scatter(true_all[:, i], pred_all[:, i], alpha=0.6, s=20)
        plt.plot([true_all[:, i].min(), true_all[:, i].max()],
                 [true_all[:, i].min(), true_all[:, i].max()], 'r--')
        plt.xlabel(f'True {param_names[i]}')
        plt.ylabel(f'Predicted {param_names[i]}')
        plt.title(f'{param_names[i]}: MSE={overall_mse[i]:.4f}, Bias={overall_bias[i]:.4f}')
        plt.grid(True, alpha=0.3)

    # 不确定性分布
    plt.subplot(2, 3, 6)
    epistemic_mean = np.mean(epistemic_all, axis=1)
    aleatoric_mean = np.mean(aleatoric_all, axis=1)
    plt.scatter(epistemic_mean, aleatoric_mean, alpha=0.6, s=20)
    plt.xlabel('Average Epistemic Uncertainty')
    plt.ylabel('Average Aleatoric Uncertainty')
    plt.title('Uncertainty Distribution')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("./results/bnn_analysis_optimized.png", dpi=300, bbox_inches='tight')
    plt.close()

    print(f"\nAll results saved to ./results/")
    print("Training completed successfully!")
